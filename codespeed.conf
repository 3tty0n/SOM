# Config file for ReBench
# Config format is YAML (see http://yaml.org/ for detailed spec)

# this run definition will be choosen if no parameters are given to rebench.py
standard_run: all
standard_data_file: 'codespeed.data'

reporting:
    # results can also be reported to a codespeed instance
    # see: https://github.com/tobami/codespeed
    csv_file: latest-runs.csv
    csv_locale: de_DE.UTF-8
#    codespeed:
#        url: http://soft.vub.ac.be/~ppp/codespeed/result/add/json/
#        project: SOM

# settings and requirements for statistic evaluation
statistics:
    min_runs: 5
    max_runs: 10
    confidence_level: 0.90
    error_margin: 0.005
 
# settings for quick runs, useful for fast feedback during experiments
quick_runs:
    min_runs: 3
    max_runs: 10
    max_time: 60   # time in seconds

# definition of benchmark suites
benchmark_suites:
    macro-startup:
        performance_reader: LogPerformance
        command: " %(benchmark)s "
        #input_sizes:
        #    - benchmarking.image ReBenchHarness
        max_runtime: 200
        benchmarks:
            - 1

    macro-steady:
        performance_reader: LogPerformance
        command: " %(benchmark)s "
        #input_sizes:
        #    - benchmarking.image ReBenchHarness
        max_runtime: 200
        benchmarks:
            - 1

    micro-startup:
        performance_reader: LogPerformance
        command: "Examples/Benchmarks/BenchmarkHarness.som %(benchmark)s "
        #input_sizes:
        #    - benchmarking.image ReBenchHarness
        max_runtime: 200
        benchmarks:
            - Fibonacci:
                extra_args: "1 0 300"
            - Dispatch:
                extra_args: "1 0 2000"
            - Bounce:
                extra_args: "1 0 200"
            - Loop:
                extra_args: "1 0 1000"
            - Permute:
                extra_args: "1 0 300"
            - Queens:
                extra_args: "1 0 200"
            - List:
                extra_args: "1 0 200"
            - Recurse:
                extra_args: "1 0 300"
            - Storage:
                extra_args: "1 0 150"
            - Sieve:
                extra_args: "1 0 500"
            - BubbleSort:
                extra_args: "1 0 300"
            - QuickSort:
                extra_args: "1 0 300"
            - Sum:
                extra_args: "1 0 1000"
            - Towers:
                extra_args: "1 0 200"
            - TreeSort:
                extra_args: "1 0 100"
            - IntegerLoop:
                extra_args: "1 0 800"
            - FieldLoop:
                extra_args: "1 0 300"

    micro-steady:
        performance_reader: LogPerformance
        command: "Examples/Benchmarks/BenchmarkHarness.som %(benchmark)s "
        #input_sizes:
        #    - benchmarking.image ReBenchHarness
        max_runtime: 200
        benchmarks:
            - Fibonacci:
                extra_args: "1 2 300"
            - Dispatch:
                extra_args: "1 2 2000"
            - Bounce:
                extra_args: "1 2 200"
            - Loop:
                extra_args: "1 2 1000"
            - Permute:
                extra_args: "1 2 300"
            - Queens:
                extra_args: "1 2 200"
            - List:
                extra_args: "1 2 200"
            - Recurse:
                extra_args: "1 2 300"
            - Storage:
                extra_args: "1 2 150"
            - Sieve:
                extra_args: "1 2 500"
            - BubbleSort:
                extra_args: "1 2 300"
            - QuickSort:
                extra_args: "1 2 300"
            - Sum:
                extra_args: "1 2 1000"
            - Towers:
                extra_args: "1 2 200"
            - TreeSort:
                extra_args: "1 2 100"
            - IntegerLoop:
                extra_args: "1 2 800"
            - FieldLoop:
                extra_args: "1 2 300"

# VMs have a name and are specified by a path and the binary to be executed
virtual_machines:
    SOM:
        path: SOM
        binary: som.sh
        args: ""
    TruffleSOM-interpreter:
        path: TruffleSOM
        binary: som.sh
        args: ""
    TruffleSOM-graal:
        path: TruffleSOM
        binary: som.sh
        args: ""
    CSOM:
        path: CSOM
        binary: SOM
        args: ""
    SOMpp:
        path: SOM
        binary: SOM++
        args: ""
    PySOM:
        path: .
        binary: PySOM/som.sh
        args: "-cp PySOM/Smalltalk "
    RPySOM-interpreter:
        path: .
        binary: RPySOM/som
        args: "-cp RPySOM/Smalltalk "
    RPySOM-jit:
        path: .
        binary: RPySOM/som
        args: "-cp RPySOM/Smalltalk "
        
# define the benchmarks to be executed for a re-executable benchmark run
run_definitions:
    SOM:
        description: All benchmarks on SOM (Java, bytecode-based)
        actions: benchmark
        benchmark:
            - micro-startup
            - micro-steady
            #- macro-startup
            #- macro-steady
        executions: SOM
    TruffleSOM:
        description: All benchmarks on TruffleSOM (Java, AST Interpreter)
        actions: benchmark
        benchmark:
            - micro-startup
            - micro-steady
            #- macro-startup
            #- macro-steady
        executions:
            - TruffleSOM-interpreter
            - TruffleSOM-graal
    CSOM:
        description: All benchmarks on CSOM
        actions: benchmark
        benchmark:
            - micro-startup
            #- macro-startup
        executions:
            - CSOM
    SOMpp:
        description: All benchmarks on SOM++
        actions: benchmark
        benchmark:
            - micro-startup
            #- macro-startup
        executions:
            - SOMpp
    PySOM:
        description: All benchmarks on PySOM
        actions: benchmark
        benchmark:
            - micro-startup
            #- macro-startup
        executions:
            - PySOM
    RPySOM:
        description: All benchmarks on RPySOM
        actions: benchmark
        benchmark:
            - micro-startup
            - micro-steady
            #- macro-startup
            #- macro-steady
        executions:
            #- RPySOM-interpreter
            - RPySOM-jit
